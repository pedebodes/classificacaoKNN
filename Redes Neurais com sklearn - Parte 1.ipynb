{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "print(len(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18795672\n",
      "Iteration 2, loss = 1.17021522\n",
      "Iteration 3, loss = 1.15363403\n",
      "Iteration 4, loss = 1.13829910\n",
      "Iteration 5, loss = 1.12428364\n",
      "Iteration 6, loss = 1.11164396\n",
      "Iteration 7, loss = 1.10039467\n",
      "Iteration 8, loss = 1.09047283\n",
      "Iteration 9, loss = 1.08171414\n",
      "Iteration 10, loss = 1.07387242\n",
      "Iteration 11, loss = 1.06667804\n",
      "Iteration 12, loss = 1.05988899\n",
      "Iteration 13, loss = 1.05331480\n",
      "Iteration 14, loss = 1.04682002\n",
      "Iteration 15, loss = 1.04031678\n",
      "Iteration 16, loss = 1.03375414\n",
      "Iteration 17, loss = 1.02710779\n",
      "Iteration 18, loss = 1.02037102\n",
      "Iteration 19, loss = 1.01354723\n",
      "Iteration 20, loss = 1.00664388\n",
      "Iteration 21, loss = 0.99966771\n",
      "Iteration 22, loss = 0.99262129\n",
      "Iteration 23, loss = 0.98550097\n",
      "Iteration 24, loss = 0.97829650\n",
      "Iteration 25, loss = 0.97099204\n",
      "Iteration 26, loss = 0.96356846\n",
      "Iteration 27, loss = 0.95600640\n",
      "Iteration 28, loss = 0.94828930\n",
      "Iteration 29, loss = 0.94040578\n",
      "Iteration 30, loss = 0.93235079\n",
      "Iteration 31, loss = 0.92412534\n",
      "Iteration 32, loss = 0.91573487\n",
      "Iteration 33, loss = 0.90718673\n",
      "Iteration 34, loss = 0.89848750\n",
      "Iteration 35, loss = 0.88964087\n",
      "Iteration 36, loss = 0.88064688\n",
      "Iteration 37, loss = 0.87150251\n",
      "Iteration 38, loss = 0.86220349\n",
      "Iteration 39, loss = 0.85274653\n",
      "Iteration 40, loss = 0.84313148\n",
      "Iteration 41, loss = 0.83336289\n",
      "Iteration 42, loss = 0.82345086\n",
      "Iteration 43, loss = 0.81341118\n",
      "Iteration 44, loss = 0.80326460\n",
      "Iteration 45, loss = 0.79303566\n",
      "Iteration 46, loss = 0.78275112\n",
      "Iteration 47, loss = 0.77243830\n",
      "Iteration 48, loss = 0.76212349\n",
      "Iteration 49, loss = 0.75183046\n",
      "Iteration 50, loss = 0.74157930\n",
      "Iteration 51, loss = 0.73138546\n",
      "Iteration 52, loss = 0.72125930\n",
      "Iteration 53, loss = 0.71120609\n",
      "Iteration 54, loss = 0.70122664\n",
      "Iteration 55, loss = 0.69131870\n",
      "Iteration 56, loss = 0.68147882\n",
      "Iteration 57, loss = 0.67170441\n",
      "Iteration 58, loss = 0.66199532\n",
      "Iteration 59, loss = 0.65235447\n",
      "Iteration 60, loss = 0.64278794\n",
      "Iteration 61, loss = 0.63330561\n",
      "Iteration 62, loss = 0.62392196\n",
      "Iteration 63, loss = 0.61465573\n",
      "Iteration 64, loss = 0.60552835\n",
      "Iteration 65, loss = 0.59656226\n",
      "Iteration 66, loss = 0.58777931\n",
      "Iteration 67, loss = 0.57919928\n",
      "Iteration 68, loss = 0.57083858\n",
      "Iteration 69, loss = 0.56270910\n",
      "Iteration 70, loss = 0.55481740\n",
      "Iteration 71, loss = 0.54716438\n",
      "Iteration 72, loss = 0.53974556\n",
      "Iteration 73, loss = 0.53255201\n",
      "Iteration 74, loss = 0.52557152\n",
      "Iteration 75, loss = 0.51878991\n",
      "Iteration 76, loss = 0.51219208\n",
      "Iteration 77, loss = 0.50576282\n",
      "Iteration 78, loss = 0.49948737\n",
      "Iteration 79, loss = 0.49335164\n",
      "Iteration 80, loss = 0.48734229\n",
      "Iteration 81, loss = 0.48144670\n",
      "Iteration 82, loss = 0.47565294\n",
      "Iteration 83, loss = 0.46994991\n",
      "Iteration 84, loss = 0.46432754\n",
      "Iteration 85, loss = 0.45877701\n",
      "Iteration 86, loss = 0.45329080\n",
      "Iteration 87, loss = 0.44786258\n",
      "Iteration 88, loss = 0.44248692\n",
      "Iteration 89, loss = 0.43715917\n",
      "Iteration 90, loss = 0.43187554\n",
      "Iteration 91, loss = 0.42663333\n",
      "Iteration 92, loss = 0.42143091\n",
      "Iteration 93, loss = 0.41626744\n",
      "Iteration 94, loss = 0.41114241\n",
      "Iteration 95, loss = 0.40605533\n",
      "Iteration 96, loss = 0.40100566\n",
      "Iteration 97, loss = 0.39599308\n",
      "Iteration 98, loss = 0.39101766\n",
      "Iteration 99, loss = 0.38607986\n",
      "Iteration 100, loss = 0.38118030\n",
      "Iteration 101, loss = 0.37631977\n",
      "Iteration 102, loss = 0.37149928\n",
      "Iteration 103, loss = 0.36672013\n",
      "Iteration 104, loss = 0.36198388\n",
      "Iteration 105, loss = 0.35729212\n",
      "Iteration 106, loss = 0.35264635\n",
      "Iteration 107, loss = 0.34804796\n",
      "Iteration 108, loss = 0.34349836\n",
      "Iteration 109, loss = 0.33899895\n",
      "Iteration 110, loss = 0.33455110\n",
      "Iteration 111, loss = 0.33015605\n",
      "Iteration 112, loss = 0.32581499\n",
      "Iteration 113, loss = 0.32152909\n",
      "Iteration 114, loss = 0.31729952\n",
      "Iteration 115, loss = 0.31312731\n",
      "Iteration 116, loss = 0.30901337\n",
      "Iteration 117, loss = 0.30495843\n",
      "Iteration 118, loss = 0.30096314\n",
      "Iteration 119, loss = 0.29702803\n",
      "Iteration 120, loss = 0.29315351\n",
      "Iteration 121, loss = 0.28933983\n",
      "Iteration 122, loss = 0.28558718\n",
      "Iteration 123, loss = 0.28189566\n",
      "Iteration 124, loss = 0.27826532\n",
      "Iteration 125, loss = 0.27469608\n",
      "Iteration 126, loss = 0.27118780\n",
      "Iteration 127, loss = 0.26774028\n",
      "Iteration 128, loss = 0.26435321\n",
      "Iteration 129, loss = 0.26102625\n",
      "Iteration 130, loss = 0.25775895\n",
      "Iteration 131, loss = 0.25455083\n",
      "Iteration 132, loss = 0.25140135\n",
      "Iteration 133, loss = 0.24830994\n",
      "Iteration 134, loss = 0.24527599\n",
      "Iteration 135, loss = 0.24229886\n",
      "Iteration 136, loss = 0.23937787\n",
      "Iteration 137, loss = 0.23651234\n",
      "Iteration 138, loss = 0.23370153\n",
      "Iteration 139, loss = 0.23094468\n",
      "Iteration 140, loss = 0.22824102\n",
      "Iteration 141, loss = 0.22558975\n",
      "Iteration 142, loss = 0.22299008\n",
      "Iteration 143, loss = 0.22044119\n",
      "Iteration 144, loss = 0.21794225\n",
      "Iteration 145, loss = 0.21549244\n",
      "Iteration 146, loss = 0.21309094\n",
      "Iteration 147, loss = 0.21073690\n",
      "Iteration 148, loss = 0.20842950\n",
      "Iteration 149, loss = 0.20616791\n",
      "Iteration 150, loss = 0.20395128\n",
      "Iteration 151, loss = 0.20177878\n",
      "Iteration 152, loss = 0.19964960\n",
      "Iteration 153, loss = 0.19756291\n",
      "Iteration 154, loss = 0.19551790\n",
      "Iteration 155, loss = 0.19351376\n",
      "Iteration 156, loss = 0.19154969\n",
      "Iteration 157, loss = 0.18962490\n",
      "Iteration 158, loss = 0.18773861\n",
      "Iteration 159, loss = 0.18589005\n",
      "Iteration 160, loss = 0.18407845\n",
      "Iteration 161, loss = 0.18230305\n",
      "Iteration 162, loss = 0.18056311\n",
      "Iteration 163, loss = 0.17885789\n",
      "Iteration 164, loss = 0.17718667\n",
      "Iteration 165, loss = 0.17554874\n",
      "Iteration 166, loss = 0.17394338\n",
      "Iteration 167, loss = 0.17236992\n",
      "Iteration 168, loss = 0.17082767\n",
      "Iteration 169, loss = 0.16931596\n",
      "Iteration 170, loss = 0.16783413\n",
      "Iteration 171, loss = 0.16638154\n",
      "Iteration 172, loss = 0.16495755\n",
      "Iteration 173, loss = 0.16356154\n",
      "Iteration 174, loss = 0.16219290\n",
      "Iteration 175, loss = 0.16085103\n",
      "Iteration 176, loss = 0.15953534\n",
      "Iteration 177, loss = 0.15824526\n",
      "Iteration 178, loss = 0.15698021\n",
      "Iteration 179, loss = 0.15573965\n",
      "Iteration 180, loss = 0.15452304\n",
      "Iteration 181, loss = 0.15332984\n",
      "Iteration 182, loss = 0.15215953\n",
      "Iteration 183, loss = 0.15101160\n",
      "Iteration 184, loss = 0.14988556\n",
      "Iteration 185, loss = 0.14878092\n",
      "Iteration 186, loss = 0.14769720\n",
      "Iteration 187, loss = 0.14663393\n",
      "Iteration 188, loss = 0.14559065\n",
      "Iteration 189, loss = 0.14456693\n",
      "Iteration 190, loss = 0.14356232\n",
      "Iteration 191, loss = 0.14257640\n",
      "Iteration 192, loss = 0.14160875\n",
      "Iteration 193, loss = 0.14065896\n",
      "Iteration 194, loss = 0.13972664\n",
      "Iteration 195, loss = 0.13881139\n",
      "Iteration 196, loss = 0.13791284\n",
      "Iteration 197, loss = 0.13703061\n",
      "Iteration 198, loss = 0.13616435\n",
      "Iteration 199, loss = 0.13531369\n",
      "Iteration 200, loss = 0.13447830\n",
      "Iteration 201, loss = 0.13365783\n",
      "Iteration 202, loss = 0.13285196\n",
      "Iteration 203, loss = 0.13206036\n",
      "Iteration 204, loss = 0.13128272\n",
      "Iteration 205, loss = 0.13051874\n",
      "Iteration 206, loss = 0.12976811\n",
      "Iteration 207, loss = 0.12903054\n",
      "Iteration 208, loss = 0.12830575\n",
      "Iteration 209, loss = 0.12759346\n",
      "Iteration 210, loss = 0.12689339\n",
      "Iteration 211, loss = 0.12620529\n",
      "Iteration 212, loss = 0.12552889\n",
      "Iteration 213, loss = 0.12486395\n",
      "Iteration 214, loss = 0.12421021\n",
      "Iteration 215, loss = 0.12356743\n",
      "Iteration 216, loss = 0.12293539\n",
      "Iteration 217, loss = 0.12231385\n",
      "Iteration 218, loss = 0.12170259\n",
      "Iteration 219, loss = 0.12110139\n",
      "Iteration 220, loss = 0.12051004\n",
      "Iteration 221, loss = 0.11992833\n",
      "Iteration 222, loss = 0.11935606\n",
      "Iteration 223, loss = 0.11879303\n",
      "Iteration 224, loss = 0.11823905\n",
      "Iteration 225, loss = 0.11769393\n",
      "Iteration 226, loss = 0.11715750\n",
      "Iteration 227, loss = 0.11662956\n",
      "Iteration 228, loss = 0.11610994\n",
      "Iteration 229, loss = 0.11559848\n",
      "Iteration 230, loss = 0.11509500\n",
      "Iteration 231, loss = 0.11459935\n",
      "Iteration 232, loss = 0.11411137\n",
      "Iteration 233, loss = 0.11363089\n",
      "Iteration 234, loss = 0.11315778\n",
      "Iteration 235, loss = 0.11269188\n",
      "Iteration 236, loss = 0.11223305\n",
      "Iteration 237, loss = 0.11178115\n",
      "Iteration 238, loss = 0.11133603\n",
      "Iteration 239, loss = 0.11089758\n",
      "Iteration 240, loss = 0.11046565\n",
      "Iteration 241, loss = 0.11004012\n",
      "Iteration 242, loss = 0.10962086\n",
      "Iteration 243, loss = 0.10920776\n",
      "Iteration 244, loss = 0.10880068\n",
      "Iteration 245, loss = 0.10839953\n",
      "Iteration 246, loss = 0.10800417\n",
      "Iteration 247, loss = 0.10761451\n",
      "Iteration 248, loss = 0.10723043\n",
      "Iteration 249, loss = 0.10685183\n",
      "Iteration 250, loss = 0.10647860\n",
      "Iteration 251, loss = 0.10611065\n",
      "Iteration 252, loss = 0.10574787\n",
      "Iteration 253, loss = 0.10539017\n",
      "Iteration 254, loss = 0.10503744\n",
      "Iteration 255, loss = 0.10468962\n",
      "Iteration 256, loss = 0.10434659\n",
      "Iteration 257, loss = 0.10400827\n",
      "Iteration 258, loss = 0.10367458\n",
      "Iteration 259, loss = 0.10334543\n",
      "Iteration 260, loss = 0.10302074\n",
      "Iteration 261, loss = 0.10270043\n",
      "Iteration 262, loss = 0.10238443\n",
      "Iteration 263, loss = 0.10207264\n",
      "Iteration 264, loss = 0.10176500\n",
      "Iteration 265, loss = 0.10146143\n",
      "Iteration 266, loss = 0.10116187\n",
      "Iteration 267, loss = 0.10086624\n",
      "Iteration 268, loss = 0.10057447\n",
      "Iteration 269, loss = 0.10028649\n",
      "Iteration 270, loss = 0.10000225\n",
      "Iteration 271, loss = 0.09972166\n",
      "Iteration 272, loss = 0.09944468\n",
      "Iteration 273, loss = 0.09917123\n",
      "Iteration 274, loss = 0.09890127\n",
      "Iteration 275, loss = 0.09863472\n",
      "Iteration 276, loss = 0.09837153\n",
      "Iteration 277, loss = 0.09811164\n",
      "Iteration 278, loss = 0.09785500\n",
      "Iteration 279, loss = 0.09760155\n",
      "Iteration 280, loss = 0.09735124\n",
      "Iteration 281, loss = 0.09710402\n",
      "Iteration 282, loss = 0.09685984\n",
      "Iteration 283, loss = 0.09661863\n",
      "Iteration 284, loss = 0.09638037\n",
      "Iteration 285, loss = 0.09614499\n",
      "Iteration 286, loss = 0.09591245\n",
      "Iteration 287, loss = 0.09568270\n",
      "Iteration 288, loss = 0.09545570\n",
      "Iteration 289, loss = 0.09523141\n",
      "Iteration 290, loss = 0.09500978\n",
      "Iteration 291, loss = 0.09479076\n",
      "Iteration 292, loss = 0.09457432\n",
      "Iteration 293, loss = 0.09436041\n",
      "Iteration 294, loss = 0.09414900\n",
      "Iteration 295, loss = 0.09394004\n",
      "Iteration 296, loss = 0.09373349\n",
      "Iteration 297, loss = 0.09352933\n",
      "Iteration 298, loss = 0.09332750\n",
      "Iteration 299, loss = 0.09312798\n",
      "Iteration 300, loss = 0.09293072\n",
      "Iteration 301, loss = 0.09273570\n",
      "Iteration 302, loss = 0.09254288\n",
      "Iteration 303, loss = 0.09235222\n",
      "Iteration 304, loss = 0.09216369\n",
      "Iteration 305, loss = 0.09197726\n",
      "Iteration 306, loss = 0.09179289\n",
      "Iteration 307, loss = 0.09161056\n",
      "Iteration 308, loss = 0.09143023\n",
      "Iteration 309, loss = 0.09125188\n",
      "Iteration 310, loss = 0.09107547\n",
      "Iteration 311, loss = 0.09090098\n",
      "Iteration 312, loss = 0.09072837\n",
      "Iteration 313, loss = 0.09055762\n",
      "Iteration 314, loss = 0.09038870\n",
      "Iteration 315, loss = 0.09022159\n",
      "Iteration 316, loss = 0.09005625\n",
      "Iteration 317, loss = 0.08989266\n",
      "Iteration 318, loss = 0.08973079\n",
      "Iteration 319, loss = 0.08957063\n",
      "Iteration 320, loss = 0.08941214\n",
      "Iteration 321, loss = 0.08925530\n",
      "Iteration 322, loss = 0.08910009\n",
      "Iteration 323, loss = 0.08894648\n",
      "Iteration 324, loss = 0.08879445\n",
      "Iteration 325, loss = 0.08864398\n",
      "Iteration 326, loss = 0.08849504\n",
      "Iteration 327, loss = 0.08834762\n",
      "Iteration 328, loss = 0.08820169\n",
      "Iteration 329, loss = 0.08805722\n",
      "Iteration 330, loss = 0.08791421\n",
      "Iteration 331, loss = 0.08777263\n",
      "Iteration 332, loss = 0.08763246\n",
      "Iteration 333, loss = 0.08749367\n",
      "Iteration 334, loss = 0.08735626\n",
      "Iteration 335, loss = 0.08722019\n",
      "Iteration 336, loss = 0.08708546\n",
      "Iteration 337, loss = 0.08695204\n",
      "Iteration 338, loss = 0.08681992\n",
      "Iteration 339, loss = 0.08668908\n",
      "Iteration 340, loss = 0.08655949\n",
      "Iteration 341, loss = 0.08643115\n",
      "Iteration 342, loss = 0.08630403\n",
      "Iteration 343, loss = 0.08617813\n",
      "Iteration 344, loss = 0.08605341\n",
      "Iteration 345, loss = 0.08592988\n",
      "Iteration 346, loss = 0.08580750\n",
      "Iteration 347, loss = 0.08568627\n",
      "Iteration 348, loss = 0.08556616\n",
      "Iteration 349, loss = 0.08544717\n",
      "Iteration 350, loss = 0.08532928\n",
      "Iteration 351, loss = 0.08521248\n",
      "Iteration 352, loss = 0.08509675\n",
      "Iteration 353, loss = 0.08498207\n",
      "Iteration 354, loss = 0.08486843\n",
      "Iteration 355, loss = 0.08475583\n",
      "Iteration 356, loss = 0.08464423\n",
      "Iteration 357, loss = 0.08453364\n",
      "Iteration 358, loss = 0.08442404\n",
      "Iteration 359, loss = 0.08431541\n",
      "Iteration 360, loss = 0.08420775\n",
      "Iteration 361, loss = 0.08410104\n",
      "Iteration 362, loss = 0.08399526\n",
      "Iteration 363, loss = 0.08389041\n",
      "Iteration 364, loss = 0.08378648\n",
      "Iteration 365, loss = 0.08368344\n",
      "Iteration 366, loss = 0.08358130\n",
      "Iteration 367, loss = 0.08348003\n",
      "Iteration 368, loss = 0.08337964\n",
      "Iteration 369, loss = 0.08328010\n",
      "Iteration 370, loss = 0.08318141\n",
      "Iteration 371, loss = 0.08308355\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "-----------------------------------------------------------\n",
      "Saída da rede:\t [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 2 2 1]\n",
      "Saída desejada:\t [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 2 2 1]\n",
      "-----------------------------------------------------------\n",
      "Score:  1.0\n",
      "Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#entrada e saida\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "#calisificador mult layer interceptron \n",
    "# Documentação \n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', alpha=0.0001, hidden_layer_sizes=(5,), random_state=1,\n",
    "                       learning_rate='constant', learning_rate_init=0.01, max_iter=500,\n",
    "                       activation='logistic', momentum=0.9, verbose=True, tol=0.0001)\n",
    "\n",
    "# divisão do dataset para teste e treino\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "#treinar o classificador\n",
    "mlp.fit(X_treino, y_treino)\n",
    "\n",
    "#classificar utuilizando o predict\n",
    "saidas = mlp.predict(X_teste)\n",
    "\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "print('Saída da rede:\\t', saidas)\n",
    "print('Saída desejada:\\t', y_teste)\n",
    "\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "'''sem imprimir 1 = 100%'''\n",
    "print('Score: ', (saidas == y_teste).sum() / len(X_teste)) \n",
    "print('Score: ', mlp.score(X_teste, y_teste))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
